{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "print(\"check\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boKc361Jm3Zq",
        "outputId": "67239d1b-efb4-4cef-d882-62036a52a3af"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "check\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xLpjC8HOJcY",
        "outputId": "b57e9589-a88a-4b11-9446-33ce9114474f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m937.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFB1yz9OMtoR",
        "outputId": "f9ba9907-c11a-4efe-8808-f37cb70338d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JCyQruQaMcIB"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(model, optimizer, epoch, batch_idx, path=\"checkpoint.pt\"):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'batch_idx': batch_idx,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict()\n",
        "    }, path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(model, optimizer, path=\"checkpoint.pt\"):\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    return checkpoint['epoch'], checkpoint['batch_idx']\n"
      ],
      "metadata": {
        "id": "fy91a0qjNiiO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "\n",
        "data = torch.load(\n",
        "    \"/content/drive/MyDrive/sentiment_analysis/dataset/tokenized_with_labels.pt\",\n",
        "    weights_only=False\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F4BYWtakNjVU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build a map from original â†’ contiguous\n",
        "# label_map = {lab: i for i, lab in enumerate(sorted(unique_labels.tolist()))}\n",
        "label_map = {0: 0, 1: 1, 3: 2}\n",
        "\n",
        "# Apply it to your dataset\n",
        "for item in data:\n",
        "    item[\"labels\"] = torch.tensor(label_map[item[\"labels\"].item()], dtype=torch.long)\n",
        "\n",
        "# Now the max label is 2, so\n",
        "output_dim = len(label_map)  # = 3\n"
      ],
      "metadata": {
        "id": "nRP733qusOfl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "N2BN3A7EsdeB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2. Basic checks\n",
        "print(\"Length of dataset:\", len(data))\n",
        "print(\"Type of first item:\", type(data[0]))\n",
        "\n",
        "# 3. Print the first item (sample)\n",
        "print(\"First item:\", data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMAURgKmOGfX",
        "outputId": "42311ac1-5ccc-405e-e579-2c274b4a2222"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset: 31726\n",
            "Type of first item: <class 'dict'>\n",
            "First item: {'input_ids': tensor([     2,   4384,   4285,   9545,     12,   3496,   1134, 109832,    477,\n",
            "         26697, 134087,   7425,   2700,   4384,     31,   8351,     37,   2863,\n",
            "          6511,  38637,   1883,   4285,   1640,   3496,   1134, 109832,     15,\n",
            "         22186,   6169,    494,  22186,      3,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(2)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "# Load tokenized data\n",
        "# tokenized_dataset = torch.load(\"/content/drive/MyDrive/sentiment_analysis/dataset/tokenized_by_indicbert_data_combined.pt\")\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data  # list of dicts\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        return {\n",
        "            'input_ids': item['input_ids'],\n",
        "            'attention_mask': item['attention_mask'],\n",
        "            'label': item['labels']\n",
        "        }\n",
        "\n",
        "\n",
        "dataset = MyDataset(data)\n",
        "# dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n"
      ],
      "metadata": {
        "id": "Y-vl6C_8Psvz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nj6v2Y9WwJGW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "# 80% training, 20% validation\n",
        "train_size = int(0.8 * len(data))\n",
        "val_size = len(data) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n"
      ],
      "metadata": {
        "id": "NJSy5JhavvJN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LO5KAp-wuQj",
        "outputId": "4e21e841-f231-467d-c3ad-d109578fbf50"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataset.Subset at 0x7d60f80e7050>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train_dataset[0]))\n",
        "print(dir(train_dataset[0]))  # See what's inside"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8H9LNatJwzeE",
        "outputId": "e2fb8509-9d53-4733-b803-2ce6ddc64113"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "['__class__', '__class_getitem__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__ior__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__or__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__ror__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'clear', 'copy', 'fromkeys', 'get', 'items', 'keys', 'pop', 'popitem', 'setdefault', 'update', 'values']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)"
      ],
      "metadata": {
        "id": "C5WrzQU0wLj8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_features, train_labels = next(iter(dataloader))"
      ],
      "metadata": {
        "id": "kyt2RbLIoVj1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, optimizer, epoch, batch_idx, path=\"checkpoint.pt\"):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'batch_idx': batch_idx,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict()\n",
        "    }, path)\n"
      ],
      "metadata": {
        "id": "rYjkHnEaP4cL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(model, optimizer, path=\"checkpoint.pt\"):\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    return checkpoint['epoch'], checkpoint['batch_idx']\n"
      ],
      "metadata": {
        "id": "_SMlYgotkeZ6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SentimentLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim,\n",
        "                 n_layers=1, bidirectional=False, dropout=0.3, pad_idx=0):\n",
        "        super(SentimentLSTM, self).__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=n_layers,\n",
        "            bidirectional=bidirectional,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if n_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Fully connected output\n",
        "        lstm_output_dim = hidden_dim * (2 if bidirectional else 1)\n",
        "        self.fc = nn.Linear(lstm_output_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # input_ids: (batch_size, seq_len)\n",
        "        embedded = self.dropout(self.embedding(input_ids))  # (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        # Optionally apply attention mask: lengths from mask\n",
        "        if attention_mask is not None:\n",
        "            lengths = attention_mask.sum(dim=1).cpu()\n",
        "            packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
        "            packed_output, (hidden, _) = self.lstm(packed)\n",
        "        else:\n",
        "            output, (hidden, _) = self.lstm(embedded)\n",
        "\n",
        "        # If bidirectional, concatenate last forward and backward hidden states\n",
        "        if self.lstm.bidirectional:\n",
        "            hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
        "        else:\n",
        "            hidden = hidden[-1]\n",
        "\n",
        "        out = self.fc(self.dropout(hidden))  # (batch_size, output_dim)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "uqkTsVEyph-g"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    device = next(model.parameters()).device  # Get model's current device\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            inputs = batch['input_ids'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Get predictions\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = correct / total if total > 0 else 0.0\n",
        "\n",
        "    print(f\"Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "    model.train()\n",
        "    return avg_loss\n"
      ],
      "metadata": {
        "id": "gvN1cLcbrYow"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, delta=0.0):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = float('inf')\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if val_loss < self.best_loss - self.delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n"
      ],
      "metadata": {
        "id": "854_-185rOgH"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 200000              # For BERT/IndicBERT vocab (if using tokenizer vocab)\n",
        "embedding_dim = 128             # Smaller if not using pretrained embeddings\n",
        "hidden_dim = 256\n",
        "output_dim = 3                  # e.g., for 4 sentiment classes\n",
        "n_layers = 10\n",
        "bidirectional = True\n",
        "dropout = 0.3\n",
        "pad_idx = 0                     # Usually the token ID of [PAD]\n",
        "num_epochs=20\n",
        "\n",
        "model = SentimentLSTM(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    hidden_dim=hidden_dim,\n",
        "    output_dim=output_dim,\n",
        "    n_layers=n_layers,\n",
        "    bidirectional=bidirectional,\n",
        "    dropout=dropout,\n",
        "    pad_idx=pad_idx\n",
        ")\n"
      ],
      "metadata": {
        "id": "aNVwXD9dpp0o"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcYChonyLTRU",
        "outputId": "02c09645-28be-442e-9660-435e3f4e6343"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SentimentLSTM(\n",
              "  (embedding): Embedding(200000, 128, padding_idx=0)\n",
              "  (lstm): LSTM(128, 256, num_layers=10, batch_first=True, dropout=0.3, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n"
      ],
      "metadata": {
        "id": "T97E6AtJp4Aj"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "checkpoint_dir = \"/content/drive/MyDrive/sentiment_analysis/checkpoints\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "checkpoint_path = os.path.join(checkpoint_dir, \"lstm_checkpoint_2.pt\")\n"
      ],
      "metadata": {
        "id": "BhPpzde83HWy"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define your LSTM model here\n",
        "# model = SentimentLSTM()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "start_epoch, start_batch = 0, 0\n",
        "# checkpoint_path = \"checkpoint.pt\"\n",
        "\n",
        "# Try to load previous state\n",
        "try:\n",
        "    start_epoch, start_batch = load_checkpoint(model, optimizer, checkpoint_path)\n",
        "    print(f\"Resuming from epoch {start_epoch}, batch {start_batch}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Starting from scratch.\")\n",
        "\n",
        "early_stopping = EarlyStopping(patience=3)\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        if epoch == start_epoch and batch_idx < start_batch:\n",
        "            continue\n",
        "\n",
        "        # inputs = batch['input_ids']\n",
        "        # labels = batch['label']\n",
        "\n",
        "        inputs = batch['input_ids'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            save_checkpoint(model, optimizer, epoch, batch_idx, checkpoint_path)\n",
        "            print(f\"Checkpoint saved at epoch {epoch}, batch {batch_idx}\")\n",
        "\n",
        "    # Run validation at end of epoch\n",
        "    val_loss = evaluate(model, val_loader, criterion)\n",
        "    print(f\"Epoch {epoch} Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "    early_stopping(val_loss)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"ğŸ›‘ Early stopping triggered.\")\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcmvN8Zin_yk",
        "outputId": "a9536d7f-9fb3-4e91-ac30-0bfb482b0d53"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting from scratch.\n",
            "Checkpoint saved at epoch 0, batch 0\n",
            "Checkpoint saved at epoch 0, batch 100\n",
            "Checkpoint saved at epoch 0, batch 200\n",
            "Checkpoint saved at epoch 0, batch 300\n",
            "Validation Loss: 0.8761, Accuracy: 64.37%\n",
            "Epoch 0 Validation Loss: 0.8761\n",
            "Checkpoint saved at epoch 1, batch 0\n",
            "Checkpoint saved at epoch 1, batch 100\n",
            "Checkpoint saved at epoch 1, batch 200\n",
            "Checkpoint saved at epoch 1, batch 300\n",
            "Validation Loss: 0.8464, Accuracy: 66.50%\n",
            "Epoch 1 Validation Loss: 0.8464\n",
            "Checkpoint saved at epoch 2, batch 0\n",
            "Checkpoint saved at epoch 2, batch 100\n",
            "Checkpoint saved at epoch 2, batch 200\n",
            "Checkpoint saved at epoch 2, batch 300\n",
            "Validation Loss: 0.7872, Accuracy: 68.04%\n",
            "Epoch 2 Validation Loss: 0.7872\n",
            "Checkpoint saved at epoch 3, batch 0\n",
            "Checkpoint saved at epoch 3, batch 100\n",
            "Checkpoint saved at epoch 3, batch 200\n",
            "Checkpoint saved at epoch 3, batch 300\n",
            "Validation Loss: 0.7664, Accuracy: 69.43%\n",
            "Epoch 3 Validation Loss: 0.7664\n",
            "Checkpoint saved at epoch 4, batch 0\n",
            "Checkpoint saved at epoch 4, batch 100\n",
            "Checkpoint saved at epoch 4, batch 200\n",
            "Checkpoint saved at epoch 4, batch 300\n",
            "Validation Loss: 0.7879, Accuracy: 69.07%\n",
            "Epoch 4 Validation Loss: 0.7879\n",
            "Checkpoint saved at epoch 5, batch 0\n",
            "Checkpoint saved at epoch 5, batch 100\n",
            "Checkpoint saved at epoch 5, batch 200\n",
            "Checkpoint saved at epoch 5, batch 300\n",
            "Validation Loss: 0.7914, Accuracy: 68.34%\n",
            "Epoch 5 Validation Loss: 0.7914\n",
            "Checkpoint saved at epoch 6, batch 0\n",
            "Checkpoint saved at epoch 6, batch 100\n",
            "Checkpoint saved at epoch 6, batch 200\n",
            "Checkpoint saved at epoch 6, batch 300\n",
            "Validation Loss: 0.7839, Accuracy: 68.70%\n",
            "Epoch 6 Validation Loss: 0.7839\n",
            "ğŸ›‘ Early stopping triggered.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def evaluate_metrics(model, dataloader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            inputs = batch['input_ids'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().tolist())\n",
        "            all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "    print(classification_report(all_labels, all_preds, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n"
      ],
      "metadata": {
        "id": "nEB7bqpx5PGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
        "test_loss = evaluate(model, test_loader, criterion)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "Nt4x9HiSORUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load tokenizer (adjust based on the one you used, e.g. \"ai4bharat/indic-bert\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load your trained model definition\n",
        "# (Make sure the SentimentLSTM class is defined as during training)\n",
        "# model = SentimentLSTM(...)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/sentiment_analysis/checkpoints/lstm_checkpoint_2.pt\")[\"model_state_dict\"])\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Label mapping\n",
        "idx_to_label = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}  # Adjust based on your data\n",
        "\n",
        "def predict(text):\n",
        "    # Tokenize the text\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Move to device\n",
        "    input_ids = encoding[\"input_ids\"].to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "        probs = nn.functional.softmax(outputs, dim=1)\n",
        "        predicted_class = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "    label = idx_to_label[predicted_class]\n",
        "    confidence = probs[0][predicted_class].item()\n",
        "\n",
        "    return label, confidence\n",
        "\n",
        "# ğŸ” Example usage\n",
        "text = \"à¤µà¥‹ à¤«à¤¼à¤¿à¤²à¥à¤® à¤¬à¥à¤°à¥€ à¤¥à¥€\"  # Replace with your sentence\n",
        "label, confidence = predict(text)\n",
        "print(f\"Predicted: {label} (Confidence: {confidence:.2f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfxSAvdwVGy9",
        "outputId": "38d3df01-ed49-483c-8aa8-f01552564e3f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: negative (Confidence: 0.95)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "72_v3CNBV5Jg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}